{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a64f4e0-c5c6-4887-a7d8-95aa685b8961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0: ëª¨ë“ˆ importë¥¼ ìœ„í•œ ê²½ë¡œ ì„¤ì •\n",
    "import os, sys\n",
    "sys.path.append(os.path.abspath(\"..\"))  # shared, models ë””ë ‰í† ë¦¬ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡ ê²½ë¡œ ì¶”ê°€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65f6f806-1698-4d95-8def-84c6326ebb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PyTorch version: 2.6.0+cu124\n",
      "ğŸš€ GPU available: True\n",
      "ğŸ§  GPU name: Quadro RTX 5000\n",
      "ğŸ’¾ Total memory: 15.73 GiB\n",
      "ğŸ“¦ Reserved memory: 0.00 GiB\n",
      "ğŸ“ˆ Allocated memory: 0.00 GiB\n",
      "ğŸŸ¢ Free memory in reserved: 0.00 GiB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: í™˜ê²½ í™•ì¸\n",
    "import torch\n",
    "\n",
    "print(f\"âœ… PyTorch version: {torch.__version__}\")\n",
    "print(f\"ğŸš€ GPU available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(f\"ğŸ§  GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    total_memory = torch.cuda.get_device_properties(device).total_memory / 1024**3  # GiB\n",
    "    reserved_memory = torch.cuda.memory_reserved(device) / 1024**3  # GiB\n",
    "    allocated_memory = torch.cuda.memory_allocated(device) / 1024**3  # GiB\n",
    "    free_memory = reserved_memory - allocated_memory  # GiB\n",
    "\n",
    "    print(f\"ğŸ’¾ Total memory: {total_memory:.2f} GiB\")\n",
    "    print(f\"ğŸ“¦ Reserved memory: {reserved_memory:.2f} GiB\")\n",
    "    print(f\"ğŸ“ˆ Allocated memory: {allocated_memory:.2f} GiB\")\n",
    "    print(f\"ğŸŸ¢ Free memory in reserved: {free_memory:.2f} GiB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1428cc1d-6b21-4463-b567-e099da347882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 20:41:42,508 | INFO | data_loader | ğŸ” Initializing dataset with 12 file pairs.\n",
      "2025-07-30 20:41:42,529 | INFO | data_loader | ğŸ“¦ Total samples across all files: 110592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Sample loaded: input shape = torch.Size([2, 1, 60, 60, 60]), output shape = torch.Size([2, 1, 60, 60, 60])\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: ë°ì´í„°ì…‹ ë¡œë”©\n",
    "from torch.utils.data import DataLoader\n",
    "from shared.data_loader import HDF5Dataset\n",
    "import os\n",
    "\n",
    "input_dir = \"/caefs/data/IllustrisTNG/subcube/input\"\n",
    "output_dir = \"/caefs/data/IllustrisTNG/subcube/output\"\n",
    "\n",
    "input_files = sorted([os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith(\".h5\")])\n",
    "output_files = sorted([os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.endswith(\".h5\")])\n",
    "\n",
    "dataset = HDF5Dataset(input_files, output_files)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "x, y = next(iter(loader))\n",
    "print(f\"âœ… Sample loaded: input shape = {x.shape}, output shape = {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2d75437-8716-4146-990e-1165e525165b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ViT model (full) loaded and set to training mode.\n",
      "ğŸ“ Dummy input: torch.Size([2, 1, 60, 60, 60]) â†’ Prediction shape: torch.Size([2, 1, 60, 60, 60])\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: ViT ì´ˆê¸°í™” for scalar regression from full 3D volume\n",
    "from models.vit.model import ViT3D\n",
    "import torch\n",
    "\n",
    "# ì…ë ¥ í¬ê¸°ì™€ ë™ì¼í•˜ê²Œ ì„¤ì •\n",
    "input_shape = (60, 60, 60)  # (D, H, W)\n",
    "patch_spatial = 10         # â†’ 60 / 10 = 6 íŒ¨ì¹˜\n",
    "patch_depth = 10           # â†’ 60 / 10 = 6 íŒ¨ì¹˜\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ëª¨ë¸ ì„ íƒ: \"full\" or \"simple\"\n",
    "model_name = \"full\"\n",
    "\n",
    "\n",
    "model = ViT3D(\n",
    "    image_size=input_shape[1],\n",
    "    frames=input_shape[0],\n",
    "    image_patch_size=patch_spatial,\n",
    "    frame_patch_size=patch_depth,\n",
    "    dim=256,\n",
    "    depth=6,\n",
    "    heads=8,\n",
    "    mlp_dim=512,\n",
    "    in_channels=1,\n",
    "    out_channels=1\n",
    ").to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Dummy input í™•ì¸\n",
    "x_dummy = torch.randn(2, 1, *input_shape).to(device)  # [B, C, D, H, W]\n",
    "y_dummy = model(x_dummy)\n",
    "\n",
    "print(f\"âœ… ViT model ({model_name}) loaded and set to training mode.\")\n",
    "print(f\"ğŸ“ Dummy input: {x_dummy.shape} â†’ Prediction shape: {y_dummy.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b1702f7-2138-40d1-9014-daf9210585b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "======================================================================================================================================================\n",
       "Layer (type:depth-idx)                             Input Shape               Output Shape              Param #                   Kernel Shape\n",
       "======================================================================================================================================================\n",
       "ViT3D                                              [2, 1, 60, 60, 60]        [2, 1, 60, 60, 60]        55,296                    --\n",
       "â”œâ”€Sequential: 1-1                                  [2, 1, 60, 60, 60]        [2, 216, 256]             --                        --\n",
       "â”‚    â””â”€Rearrange: 2-1                              [2, 1, 60, 60, 60]        [2, 216, 1000]            --                        --\n",
       "â”‚    â””â”€LayerNorm: 2-2                              [2, 216, 1000]            [2, 216, 1000]            2,000                     --\n",
       "â”‚    â””â”€Linear: 2-3                                 [2, 216, 1000]            [2, 216, 256]             256,256                   --\n",
       "â”‚    â””â”€LayerNorm: 2-4                              [2, 216, 256]             [2, 216, 256]             512                       --\n",
       "â”œâ”€Dropout: 1-2                                     [2, 216, 256]             [2, 216, 256]             --                        --\n",
       "â”œâ”€Transformer: 1-3                                 [2, 216, 256]             [2, 216, 256]             --                        --\n",
       "â”‚    â””â”€ModuleList: 2-5                             --                        --                        --                        --\n",
       "â”‚    â”‚    â””â”€ModuleList: 3-1                        --                        --                        788,480                   --\n",
       "â”‚    â”‚    â””â”€ModuleList: 3-2                        --                        --                        788,480                   --\n",
       "â”‚    â”‚    â””â”€ModuleList: 3-3                        --                        --                        788,480                   --\n",
       "â”‚    â”‚    â””â”€ModuleList: 3-4                        --                        --                        788,480                   --\n",
       "â”‚    â”‚    â””â”€ModuleList: 3-5                        --                        --                        788,480                   --\n",
       "â”‚    â”‚    â””â”€ModuleList: 3-6                        --                        --                        788,480                   --\n",
       "â”œâ”€Sequential: 1-4                                  [2, 216, 256]             [2, 216, 1000]            --                        --\n",
       "â”‚    â””â”€LayerNorm: 2-6                              [2, 216, 256]             [2, 216, 256]             512                       --\n",
       "â”‚    â””â”€Linear: 2-7                                 [2, 216, 256]             [2, 216, 1000]            257,000                   --\n",
       "======================================================================================================================================================\n",
       "Total params: 5,302,456\n",
       "Trainable params: 5,302,456\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 10.49\n",
       "======================================================================================================================================================\n",
       "Input size (MB): 1.73\n",
       "Forward/backward pass size (MB): 73.27\n",
       "Params size (MB): 20.99\n",
       "Estimated Total Size (MB): 95.98\n",
       "======================================================================================================================================================"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(2, 1, 60, 60, 60), col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79b55eac-40dd-4763-bc3c-ba18605f9b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Success with batch_size=32, pred.shape=torch.Size([32, 1, 60, 60, 60])\n",
      "âœ… Success with batch_size=16, pred.shape=torch.Size([16, 1, 60, 60, 60])\n",
      "âœ… Success with batch_size=8, pred.shape=torch.Size([8, 1, 60, 60, 60])\n",
      "âœ… Success with batch_size=4, pred.shape=torch.Size([4, 1, 60, 60, 60])\n",
      "âœ… Success with batch_size=2, pred.shape=torch.Size([2, 1, 60, 60, 60])\n",
      "âœ… Success with batch_size=1, pred.shape=torch.Size([1, 1, 60, 60, 60])\n"
     ]
    }
   ],
   "source": [
    "from models.vit.model import ViT3D\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def test_batch_size(batch_size):\n",
    "    try:\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # âš ï¸ ViT3Dì˜ patch í¬ê¸° â†’ 10ì´ë©´ 60 % 10 == 0\n",
    "        model = ViT3D(\n",
    "            image_size=60,           # H/W\n",
    "            frames=60,               # D\n",
    "            image_patch_size=10,\n",
    "            frame_patch_size=10,\n",
    "            dim=256,\n",
    "            depth=6,\n",
    "            heads=8,\n",
    "            mlp_dim=512,\n",
    "            in_channels=1,\n",
    "            out_channels=1\n",
    "        ).cuda()\n",
    "\n",
    "        for x, y in loader:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "            pred = model(x)\n",
    "            print(f\"âœ… Success with batch_size={batch_size}, pred.shape={pred.shape}\")\n",
    "            break\n",
    "    except RuntimeError as e:\n",
    "        print(f\"âŒ Failed with batch_size={batch_size}: {str(e).splitlines()[0]}\")\n",
    "\n",
    "for bs in [32, 16, 8, 4, 2, 1]:\n",
    "    test_batch_size(bs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7e39385-05d1-41b1-923d-fc1f089dcfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MSE Loss on batch: 89.6607\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: ì†ì‹¤ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸\n",
    "from shared.losses import mse_loss\n",
    "\n",
    "loss_val = mse_loss(x.to(device), y.to(device))\n",
    "print(f\"âœ… MSE Loss on batch: {loss_val.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3120991-5adc-4bed-b834-f6dfa622a2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Optimizer and LR scheduler initialized.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Optimizer ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "print(\"âœ… Optimizer and LR scheduler initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb1e522e-7997-40dd-af89-a4f8898e0869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/55296 [00:00<?, ?it/s]/caefs/user/mmingyeong/_dm2ics_model_benchmark/dm2ics_model_benchmark/shared/losses.py:34: UserWarning: Using a target size (torch.Size([2, 1])) that is different to the input size (torch.Size([2, 1, 60, 60, 60])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(pred, target)\n",
      "Epoch 1:   0%|          | 0/55296 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (60) must match the size of tensor b (2) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m optimizer.zero_grad()\n\u001b[32m     18\u001b[39m outputs = model(inputs)  \u001b[38;5;66;03m# outputs.shape: [B, 1]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m loss = \u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m loss.backward()\n\u001b[32m     21\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/caefs/user/mmingyeong/_dm2ics_model_benchmark/dm2ics_model_benchmark/shared/losses.py:34\u001b[39m, in \u001b[36mmse_loss\u001b[39m\u001b[34m(pred, target)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmse_loss\u001b[39m(pred, target):\n\u001b[32m     19\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[33;03m    Compute the Mean Squared Error (MSE) between prediction and target.\u001b[39;00m\n\u001b[32m     21\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m \u001b[33;03m        Scalar MSE loss.\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/torch/nn/functional.py:3884\u001b[39m, in \u001b[36mmse_loss\u001b[39m\u001b[34m(input, target, size_average, reduce, reduction, weight)\u001b[39m\n\u001b[32m   3881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3882\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3884\u001b[39m expanded_input, expanded_target = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3886\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3887\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m weight.size() != \u001b[38;5;28minput\u001b[39m.size():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/torch/functional.py:76\u001b[39m, in \u001b[36mbroadcast_tensors\u001b[39m\u001b[34m(*tensors)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, *tensors)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (60) must match the size of tensor b (2) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "# Cell 6: ë¹ ë¥¸ í•™ìŠµ ë£¨í”„ (1 epoch, ì¼ë¶€ batchë§Œ)\n",
    "from tqdm import tqdm\n",
    "\n",
    "model.train()\n",
    "n_batch = 10  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 10ê°œ ë°°ì¹˜ë§Œ í•™ìŠµ\n",
    "\n",
    "for epoch in range(3):\n",
    "    total_loss = 0.0\n",
    "    for i, (inputs, targets) in enumerate(tqdm(loader, desc=f\"Epoch {epoch+1}\")):\n",
    "        if i >= n_batch:\n",
    "            break\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)  # outputs.shape: [B, 1]\n",
    "        loss = mse_loss(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"ğŸ“‰ Epoch {epoch+1} (partial) Loss: {total_loss / n_batch:.4f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14af2e6-ab63-4674-8748-edfdbb8fb344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py312)",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
