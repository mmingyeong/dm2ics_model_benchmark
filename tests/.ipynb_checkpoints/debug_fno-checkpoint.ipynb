{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "051fe4bf-cb69-4dde-a560-6bd27592d048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0: 모듈 import를 위한 경로 설정\n",
    "import os, sys\n",
    "sys.path.append(os.path.abspath(\"..\"))  # shared, models 디렉토리 접근 가능하도록 경로 추가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c33f512-7bb2-485b-92b8-942cf8edfabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PyTorch version: 2.6.0+cu124\n",
      "🚀 GPU available: True\n",
      "🧠 GPU name: Quadro RTX 5000\n",
      "💾 Total memory: 15.73 GiB\n",
      "📦 Reserved memory: 0.00 GiB\n",
      "📈 Allocated memory: 0.00 GiB\n",
      "🟢 Free memory in reserved: 0.00 GiB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: 환경 확인\n",
    "import torch\n",
    "\n",
    "print(f\"✅ PyTorch version: {torch.__version__}\")\n",
    "print(f\"🚀 GPU available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(f\"🧠 GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    total_memory = torch.cuda.get_device_properties(device).total_memory / 1024**3  # GiB\n",
    "    reserved_memory = torch.cuda.memory_reserved(device) / 1024**3  # GiB\n",
    "    allocated_memory = torch.cuda.memory_allocated(device) / 1024**3  # GiB\n",
    "    free_memory = reserved_memory - allocated_memory  # GiB\n",
    "\n",
    "    print(f\"💾 Total memory: {total_memory:.2f} GiB\")\n",
    "    print(f\"📦 Reserved memory: {reserved_memory:.2f} GiB\")\n",
    "    print(f\"📈 Allocated memory: {allocated_memory:.2f} GiB\")\n",
    "    print(f\"🟢 Free memory in reserved: {free_memory:.2f} GiB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2533429-64ef-4156-90bd-a80eff88e757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 10:18:22,442 | INFO | data_loader | 🔍 Initializing dataset with 12 file pairs.\n",
      "2025-06-17 10:18:22,468 | INFO | data_loader | 📦 Total samples across all files: 110592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample loaded: input shape = torch.Size([2, 1, 60, 60, 60]), output shape = torch.Size([2, 1, 60, 60, 60])\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: 데이터셋 로딩\n",
    "from torch.utils.data import DataLoader\n",
    "from shared.data_loader import HDF5Dataset\n",
    "import os\n",
    "\n",
    "input_dir = \"/caefs/data/IllustrisTNG/subcube/input\"\n",
    "output_dir = \"/caefs/data/IllustrisTNG/subcube/output\"\n",
    "\n",
    "input_files = sorted([os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith(\".h5\")])\n",
    "output_files = sorted([os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.endswith(\".h5\")])\n",
    "\n",
    "dataset = HDF5Dataset(input_files, output_files)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "x, y = next(iter(loader))\n",
    "print(f\"✅ Sample loaded: input shape = {x.shape}, output shape = {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb82d3e3-6a0f-48ad-8349-a3c5156be53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 10:20:59,963 | INFO | models.fno.model | ✅ FNO model initialized successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FNO model loaded and set to training mode.\n"
     ]
    }
   ],
   "source": [
    "from models.fno.model import FNO\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "\n",
    "# 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# FNO 모델 초기화\n",
    "model = FNO(\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    modes1=32,\n",
    "    modes2=32,\n",
    "    modes3=32,\n",
    "    width=128,\n",
    "    lifting_channels=128,\n",
    "    add_grid=True,\n",
    "    activation=nn.ReLU\n",
    ").to(device)\n",
    "\n",
    "model.train()\n",
    "print(\"✅ FNO model loaded and set to training mode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ac286a7-e717-4a49-bf87-322ec2af49df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 10:21:00,073 | INFO | models.fno.model | 🚀 FNO forward pass started. Input shape: torch.Size([2, 1, 60, 60, 60])\n",
      "2025-06-17 10:21:00,073 | INFO | models.fno.model | 🌐 Generating coordinate grid with shape: [60, 60, 60]\n",
      "2025-06-17 10:21:00,078 | INFO | models.fno.model | ✅ Coordinate grid generated.\n",
      "2025-06-17 10:21:00,087 | INFO | models.fno.model | 🔗 Added grid to input. New shape: torch.Size([2, 4, 60, 60, 60])\n",
      "2025-06-17 10:21:00,271 | INFO | models.fno.model | 🔁 Passed through Fourier layer 1/4\n",
      "2025-06-17 10:21:00,273 | INFO | models.fno.model | 🔁 Passed through Fourier layer 2/4\n",
      "2025-06-17 10:21:00,275 | INFO | models.fno.model | 🔁 Passed through Fourier layer 3/4\n",
      "2025-06-17 10:21:00,278 | INFO | models.fno.model | 🔁 Passed through Fourier layer 4/4\n",
      "2025-06-17 10:21:00,624 | INFO | models.fno.model | ✅ Forward pass completed. Output shape: torch.Size([2, 1, 60, 60, 60])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Kernel Shape\n",
       "============================================================================================================================================\n",
       "FNO                                      [2, 1, 60, 60, 60]        [2, 1, 60, 60, 60]        --                        --\n",
       "├─Linear: 1-1                            [432000, 4]               [432000, 64]              320                       --\n",
       "├─Linear: 1-2                            [432000, 64]              [432000, 128]             8,320                     --\n",
       "├─ModuleList: 1-3                        --                        --                        --                        --\n",
       "│    └─SpectralConvolution: 2-1          [2, 128, 60, 60, 60]      [2, 128, 60, 60, 60]      70,528                    --\n",
       "│    └─SpectralConvolution: 2-2          [2, 128, 60, 60, 60]      [2, 128, 60, 60, 60]      70,528                    --\n",
       "│    └─SpectralConvolution: 2-3          [2, 128, 60, 60, 60]      [2, 128, 60, 60, 60]      70,528                    --\n",
       "│    └─SpectralConvolution: 2-4          [2, 128, 60, 60, 60]      [2, 128, 60, 60, 60]      70,528                    --\n",
       "├─Linear: 1-4                            [2, 60, 60, 60, 128]      [2, 60, 60, 60, 128]      16,512                    --\n",
       "├─Linear: 1-5                            [2, 60, 60, 60, 128]      [2, 60, 60, 60, 1]        129                       --\n",
       "============================================================================================================================================\n",
       "Total params: 307,393\n",
       "Trainable params: 307,393\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 3.73\n",
       "============================================================================================================================================\n",
       "Input size (MB): 1.73\n",
       "Forward/backward pass size (MB): 1109.38\n",
       "Params size (MB): 0.10\n",
       "Estimated Total Size (MB): 1111.21\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(2, 1, 60, 60, 60),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c932786-1525-4b31-bc13-1af777b45baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20205bcd-72cc-4c2f-a10b-54ec5689f392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 10:21:00,779 | INFO | models.fno.model | 🚀 FNO forward pass started. Input shape: torch.Size([32, 1, 60, 60, 60])\n",
      "2025-06-17 10:21:00,780 | INFO | models.fno.model | 🌐 Generating coordinate grid with shape: [60, 60, 60]\n",
      "2025-06-17 10:21:00,781 | INFO | models.fno.model | ✅ Coordinate grid generated.\n",
      "2025-06-17 10:21:00,781 | INFO | models.fno.model | 🔗 Added grid to input. New shape: torch.Size([32, 4, 60, 60, 60])\n",
      "2025-06-17 10:21:01,294 | INFO | models.fno.model | 🚀 FNO forward pass started. Input shape: torch.Size([16, 1, 60, 60, 60])\n",
      "2025-06-17 10:21:01,295 | INFO | models.fno.model | 🌐 Generating coordinate grid with shape: [60, 60, 60]\n",
      "2025-06-17 10:21:01,296 | INFO | models.fno.model | ✅ Coordinate grid generated.\n",
      "2025-06-17 10:21:01,296 | INFO | models.fno.model | 🔗 Added grid to input. New shape: torch.Size([16, 4, 60, 60, 60])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed with batch_size=32: CUDA out of memory. Tried to allocate 6.59 GiB. GPU 0 has a total capacity of 15.73 GiB of which 504.50 MiB is free. Process 12502 has 26.06 MiB memory in use. Process 133015 has 5.05 GiB memory in use. Including non-PyTorch memory, this process has 10.16 GiB memory in use. Of the allocated memory 9.95 GiB is allocated by PyTorch, and 63.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 10:21:01,669 | INFO | models.fno.model | 🚀 FNO forward pass started. Input shape: torch.Size([8, 1, 60, 60, 60])\n",
      "2025-06-17 10:21:01,670 | INFO | models.fno.model | 🌐 Generating coordinate grid with shape: [60, 60, 60]\n",
      "2025-06-17 10:21:01,670 | INFO | models.fno.model | ✅ Coordinate grid generated.\n",
      "2025-06-17 10:21:01,671 | INFO | models.fno.model | 🔗 Added grid to input. New shape: torch.Size([8, 4, 60, 60, 60])\n",
      "2025-06-17 10:21:01,751 | INFO | models.fno.model | 🔁 Passed through Fourier layer 1/4\n",
      "2025-06-17 10:21:01,753 | INFO | models.fno.model | 🔁 Passed through Fourier layer 2/4\n",
      "2025-06-17 10:21:01,755 | INFO | models.fno.model | 🔁 Passed through Fourier layer 3/4\n",
      "2025-06-17 10:21:01,757 | INFO | models.fno.model | 🔁 Passed through Fourier layer 4/4\n",
      "2025-06-17 10:21:01,758 | INFO | models.fno.model | ✅ Forward pass completed. Output shape: torch.Size([8, 1, 60, 60, 60])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed with batch_size=16: CUDA out of memory. Tried to allocate 3.30 GiB. GPU 0 has a total capacity of 15.73 GiB of which 504.50 MiB is free. Process 12502 has 26.06 MiB memory in use. Process 133015 has 5.05 GiB memory in use. Including non-PyTorch memory, this process has 10.16 GiB memory in use. Of the allocated memory 8.28 GiB is allocated by PyTorch, and 1.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "✅ Success with batch_size=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 10:21:02,554 | INFO | models.fno.model | 🚀 FNO forward pass started. Input shape: torch.Size([4, 1, 60, 60, 60])\n",
      "2025-06-17 10:21:02,555 | INFO | models.fno.model | 🌐 Generating coordinate grid with shape: [60, 60, 60]\n",
      "2025-06-17 10:21:02,556 | INFO | models.fno.model | ✅ Coordinate grid generated.\n",
      "2025-06-17 10:21:02,556 | INFO | models.fno.model | 🔗 Added grid to input. New shape: torch.Size([4, 4, 60, 60, 60])\n",
      "2025-06-17 10:21:02,601 | INFO | models.fno.model | 🔁 Passed through Fourier layer 1/4\n",
      "2025-06-17 10:21:02,604 | INFO | models.fno.model | 🔁 Passed through Fourier layer 2/4\n",
      "2025-06-17 10:21:02,606 | INFO | models.fno.model | 🔁 Passed through Fourier layer 3/4\n",
      "2025-06-17 10:21:02,608 | INFO | models.fno.model | 🔁 Passed through Fourier layer 4/4\n",
      "2025-06-17 10:21:02,609 | INFO | models.fno.model | ✅ Forward pass completed. Output shape: torch.Size([4, 1, 60, 60, 60])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success with batch_size=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 10:21:03,166 | INFO | models.fno.model | 🚀 FNO forward pass started. Input shape: torch.Size([2, 1, 60, 60, 60])\n",
      "2025-06-17 10:21:03,167 | INFO | models.fno.model | 🌐 Generating coordinate grid with shape: [60, 60, 60]\n",
      "2025-06-17 10:21:03,168 | INFO | models.fno.model | ✅ Coordinate grid generated.\n",
      "2025-06-17 10:21:03,168 | INFO | models.fno.model | 🔗 Added grid to input. New shape: torch.Size([2, 4, 60, 60, 60])\n",
      "2025-06-17 10:21:03,171 | INFO | models.fno.model | 🔁 Passed through Fourier layer 1/4\n",
      "2025-06-17 10:21:03,173 | INFO | models.fno.model | 🔁 Passed through Fourier layer 2/4\n",
      "2025-06-17 10:21:03,175 | INFO | models.fno.model | 🔁 Passed through Fourier layer 3/4\n",
      "2025-06-17 10:21:03,177 | INFO | models.fno.model | 🔁 Passed through Fourier layer 4/4\n",
      "2025-06-17 10:21:03,178 | INFO | models.fno.model | ✅ Forward pass completed. Output shape: torch.Size([2, 1, 60, 60, 60])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success with batch_size=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 10:21:03,640 | INFO | models.fno.model | 🚀 FNO forward pass started. Input shape: torch.Size([1, 1, 60, 60, 60])\n",
      "2025-06-17 10:21:03,641 | INFO | models.fno.model | 🌐 Generating coordinate grid with shape: [60, 60, 60]\n",
      "2025-06-17 10:21:03,642 | INFO | models.fno.model | ✅ Coordinate grid generated.\n",
      "2025-06-17 10:21:03,642 | INFO | models.fno.model | 🔗 Added grid to input. New shape: torch.Size([1, 4, 60, 60, 60])\n",
      "2025-06-17 10:21:03,659 | INFO | models.fno.model | 🔁 Passed through Fourier layer 1/4\n",
      "2025-06-17 10:21:03,661 | INFO | models.fno.model | 🔁 Passed through Fourier layer 2/4\n",
      "2025-06-17 10:21:03,663 | INFO | models.fno.model | 🔁 Passed through Fourier layer 3/4\n",
      "2025-06-17 10:21:03,666 | INFO | models.fno.model | 🔁 Passed through Fourier layer 4/4\n",
      "2025-06-17 10:21:03,667 | INFO | models.fno.model | ✅ Forward pass completed. Output shape: torch.Size([1, 1, 60, 60, 60])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success with batch_size=1\n"
     ]
    }
   ],
   "source": [
    "def test_batch_size(batch_size):\n",
    "    try:\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                _ = model(x)\n",
    "                print(f\"✅ Success with batch_size={batch_size}\")\n",
    "                break\n",
    "    except RuntimeError as e:\n",
    "        print(f\"❌ Failed with batch_size={batch_size}: {str(e).splitlines()[0]}\")\n",
    "\n",
    "for bs in [32, 16, 8, 4, 2, 1]:\n",
    "    test_batch_size(bs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2ac54ce-3ade-4ecd-9994-9cf5823959ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MSE Loss on sample batch: 16.1741\n"
     ]
    }
   ],
   "source": [
    "from shared.losses import mse_loss, spectral_loss\n",
    "\n",
    "loss_val = mse_loss(x.to(device), y.to(device))\n",
    "print(f\"✅ MSE Loss on sample batch: {loss_val.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc3834fd-c938-4697-80ce-c311fc22d92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Optimizer and LR scheduler initialized.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "print(\"✅ Optimizer and LR scheduler initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1de346c0-7eb5-4715-9a9d-149762f80ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/27648 [00:00<?, ?it/s]2025-06-17 10:21:06,769 | INFO | models.fno.model | 🚀 FNO forward pass started. Input shape: torch.Size([4, 1, 60, 60, 60])\n",
      "2025-06-17 10:21:06,770 | INFO | models.fno.model | 🌐 Generating coordinate grid with shape: [60, 60, 60]\n",
      "2025-06-17 10:21:06,771 | INFO | models.fno.model | ✅ Coordinate grid generated.\n",
      "2025-06-17 10:21:06,772 | INFO | models.fno.model | 🔗 Added grid to input. New shape: torch.Size([4, 4, 60, 60, 60])\n",
      "2025-06-17 10:21:06,778 | INFO | models.fno.model | 🔁 Passed through Fourier layer 1/4\n",
      "2025-06-17 10:21:06,783 | INFO | models.fno.model | 🔁 Passed through Fourier layer 2/4\n",
      "Epoch 1:   0%|          | 0/27648 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 844.00 MiB. GPU 0 has a total capacity of 15.73 GiB of which 504.50 MiB is free. Process 12502 has 26.06 MiB memory in use. Process 133015 has 5.05 GiB memory in use. Including non-PyTorch memory, this process has 10.16 GiB memory in use. Of the allocated memory 8.57 GiB is allocated by PyTorch, and 1.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m inputs, targets = inputs.to(device), targets.to(device)\n\u001b[32m     13\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m loss = mse_loss(outputs, targets)\n\u001b[32m     16\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/caefs/user/mmingyeong/_dm2ics_model_benchmark/dm2ics_model_benchmark/models/fno/model.py:137\u001b[39m, in \u001b[36mFNO.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m# Fourier layers\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.fourier_blocks):\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m🔁 Passed through Fourier layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.fourier_blocks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    140\u001b[39m \u001b[38;5;66;03m# Projection\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/caefs/user/mmingyeong/_dm2ics_model_benchmark/dm2ics_model_benchmark/models/fno/layers/spectral_convolution.py:254\u001b[39m, in \u001b[36mSpectralConvolution.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    247\u001b[39m     out_ft_real, out_ft_imag = \u001b[38;5;28mself\u001b[39m.mix_weights(\n\u001b[32m    248\u001b[39m         out_ft_real, out_ft_imag, x_ft_real, x_ft_imag,\n\u001b[32m    249\u001b[39m         tl.tt_to_tensor(\u001b[38;5;28mself\u001b[39m.factors_tt_real),\n\u001b[32m    250\u001b[39m         tl.tt_to_tensor(\u001b[38;5;28mself\u001b[39m.factors_tt_imag)\n\u001b[32m    251\u001b[39m     )\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# Combine real and imaginary parts\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m out_ft = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomplex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_ft_real\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_ft_imag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[38;5;66;03m# Apply IFFT to return to spatial domain\u001b[39;00m\n\u001b[32m    257\u001b[39m out = torch.fft.ifftn(out_ft, dim=\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mrange\u001b[39m(-\u001b[38;5;28mself\u001b[39m.dim, \u001b[32m0\u001b[39m)), s=sizes, norm=\u001b[33m'\u001b[39m\u001b[33mortho\u001b[39m\u001b[33m'\u001b[39m).real\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 844.00 MiB. GPU 0 has a total capacity of 15.73 GiB of which 504.50 MiB is free. Process 12502 has 26.06 MiB memory in use. Process 133015 has 5.05 GiB memory in use. Including non-PyTorch memory, this process has 10.16 GiB memory in use. Of the allocated memory 8.57 GiB is allocated by PyTorch, and 1.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model.train()\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "n_batch = 10\n",
    "\n",
    "for epoch in range(3):\n",
    "    total_loss = 0.0\n",
    "    for i, (inputs, targets) in enumerate(tqdm(loader, desc=f\"Epoch {epoch+1}\")):\n",
    "        if i >= n_batch:\n",
    "            break\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = mse_loss(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    print(f\"📉 Epoch {epoch+1} Loss: {total_loss / n_batch:.4f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f34bd0-297a-47bb-99f5-746fbd733181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Trainer 설정 (디버깅용)\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=2,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    log_every_n_steps=1,\n",
    "    enable_progress_bar=True,\n",
    "    detect_anomaly=True,\n",
    "    logger=CSVLogger(\"logs/debug_fno\", name=\"fno_test\"),\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=2, mode=\"min\", verbose=True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2a1fec-7c5b-4035-8ead-0697b70b60f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"fno_test_model.pt\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"✅ FNO model saved to {save_path}\")\n",
    "\n",
    "state_dict = torch.load(save_path, map_location='cpu')\n",
    "print(f\"🔍 저장된 키 개수: {len(state_dict)}\")\n",
    "print(\"예시 키:\", list(state_dict.keys())[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a209466-38d9-4d97-aef8-019b6cbb0d02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py312)",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
