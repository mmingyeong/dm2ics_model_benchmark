{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "051fe4bf-cb69-4dde-a560-6bd27592d048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0: 모듈 import를 위한 경로 설정\n",
    "import os, sys\n",
    "sys.path.append(os.path.abspath(\"..\"))  # shared, models 디렉토리 접근 가능하도록 경로 추가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c33f512-7bb2-485b-92b8-942cf8edfabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PyTorch version: 2.6.0+cu124\n",
      "🚀 GPU available: True\n",
      "🧠 GPU name: Quadro RTX 5000\n",
      "💾 Total memory: 15.73 GiB\n",
      "📦 Reserved memory: 0.00 GiB\n",
      "📈 Allocated memory: 0.00 GiB\n",
      "🟢 Free memory in reserved: 0.00 GiB\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: 환경 확인\n",
    "import torch\n",
    "\n",
    "print(f\"✅ PyTorch version: {torch.__version__}\")\n",
    "print(f\"🚀 GPU available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(f\"🧠 GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    total_memory = torch.cuda.get_device_properties(device).total_memory / 1024**3  # GiB\n",
    "    reserved_memory = torch.cuda.memory_reserved(device) / 1024**3  # GiB\n",
    "    allocated_memory = torch.cuda.memory_allocated(device) / 1024**3  # GiB\n",
    "    free_memory = reserved_memory - allocated_memory  # GiB\n",
    "\n",
    "    print(f\"💾 Total memory: {total_memory:.2f} GiB\")\n",
    "    print(f\"📦 Reserved memory: {reserved_memory:.2f} GiB\")\n",
    "    print(f\"📈 Allocated memory: {allocated_memory:.2f} GiB\")\n",
    "    print(f\"🟢 Free memory in reserved: {free_memory:.2f} GiB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2533429-64ef-4156-90bd-a80eff88e757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 16:54:49,204 | INFO | data_loader | 🔍 Initializing dataset with 12 file pairs.\n",
      "2025-06-17 16:54:49,232 | INFO | data_loader | 📦 Total samples across all files: 110592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample loaded: input shape = torch.Size([2, 1, 60, 60, 60]), output shape = torch.Size([2, 1, 60, 60, 60])\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: 데이터셋 로딩\n",
    "from torch.utils.data import DataLoader\n",
    "from shared.data_loader import HDF5Dataset\n",
    "import os\n",
    "\n",
    "input_dir = \"/caefs/data/IllustrisTNG/subcube/input\"\n",
    "output_dir = \"/caefs/data/IllustrisTNG/subcube/output\"\n",
    "\n",
    "input_files = sorted([os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith(\".h5\")])\n",
    "output_files = sorted([os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.endswith(\".h5\")])\n",
    "\n",
    "dataset = HDF5Dataset(input_files, output_files)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "x, y = next(iter(loader))\n",
    "print(f\"✅ Sample loaded: input shape = {x.shape}, output shape = {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb82d3e3-6a0f-48ad-8349-a3c5156be53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 17:22:29,945 | INFO | models.fno.model | ✅ FNO model initialized successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FNO model loaded and set to training mode.\n"
     ]
    }
   ],
   "source": [
    "from models.fno.model import FNO\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "import torch.nn as nn\n",
    "\n",
    "# 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# FNO 모델 초기화\n",
    "model = FNO(\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    modes1=32,\n",
    "    modes2=32,\n",
    "    modes3=32,\n",
    "    width=128,\n",
    "    lifting_channels=128,\n",
    "    add_grid=True,\n",
    "    activation=nn.ReLU\n",
    ").to(device)\n",
    "\n",
    "model.train()\n",
    "print(\"✅ FNO model loaded and set to training mode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ac286a7-e717-4a49-bf87-322ec2af49df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 17:22:30,059 | INFO | models.fno.model | 🚀 FNO forward pass started. Input shape: torch.Size([2, 1, 60, 60, 60])\n",
      "2025-06-17 17:22:30,060 | INFO | models.fno.model | 🌐 Generating coordinate grid with shape: [60, 60, 60]\n",
      "2025-06-17 17:22:30,065 | INFO | models.fno.model | ✅ Coordinate grid generated.\n",
      "2025-06-17 17:22:30,074 | INFO | models.fno.model | 🔗 Added grid to input. New shape: torch.Size([2, 4, 60, 60, 60])\n",
      "2025-06-17 17:22:30,760 | INFO | models.fno.model | 🔁 Passed through Fourier layer 1/4\n",
      "2025-06-17 17:22:30,763 | INFO | models.fno.model | 🔁 Passed through Fourier layer 2/4\n",
      "2025-06-17 17:22:30,766 | INFO | models.fno.model | 🔁 Passed through Fourier layer 3/4\n",
      "2025-06-17 17:22:30,769 | INFO | models.fno.model | 🔁 Passed through Fourier layer 4/4\n",
      "2025-06-17 17:22:32,680 | INFO | models.fno.model | ✅ Forward pass completed. Output shape: torch.Size([2, 1, 60, 60, 60])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Kernel Shape\n",
       "============================================================================================================================================\n",
       "FNO                                      [2, 1, 60, 60, 60]        [2, 1, 60, 60, 60]        --                        --\n",
       "├─Linear: 1-1                            [432000, 4]               [432000, 128]             640                       --\n",
       "├─Linear: 1-2                            [432000, 128]             [432000, 128]             16,512                    --\n",
       "├─ModuleList: 1-3                        --                        --                        --                        --\n",
       "│    └─SpectralConvolution: 2-1          [2, 128, 60, 60, 60]      [2, 128, 60, 60, 60]      71,296                    --\n",
       "│    └─SpectralConvolution: 2-2          [2, 128, 60, 60, 60]      [2, 128, 60, 60, 60]      71,296                    --\n",
       "│    └─SpectralConvolution: 2-3          [2, 128, 60, 60, 60]      [2, 128, 60, 60, 60]      71,296                    --\n",
       "│    └─SpectralConvolution: 2-4          [2, 128, 60, 60, 60]      [2, 128, 60, 60, 60]      71,296                    --\n",
       "├─Linear: 1-4                            [2, 60, 60, 60, 128]      [2, 60, 60, 60, 128]      16,512                    --\n",
       "├─Linear: 1-5                            [2, 60, 60, 60, 128]      [2, 60, 60, 60, 1]        129                       --\n",
       "============================================================================================================================================\n",
       "Total params: 318,977\n",
       "Trainable params: 318,977\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 7.41\n",
       "============================================================================================================================================\n",
       "Input size (MB): 1.73\n",
       "Forward/backward pass size (MB): 1330.56\n",
       "Params size (MB): 0.14\n",
       "Estimated Total Size (MB): 1332.42\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(2, 1, 60, 60, 60),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20205bcd-72cc-4c2f-a10b-54ec5689f392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 17:22:33,602 | INFO | models.fno.model | 🚀 FNO forward pass started. Input shape: torch.Size([32, 1, 60, 60, 60])\n",
      "2025-06-17 17:22:33,603 | INFO | models.fno.model | 🌐 Generating coordinate grid with shape: [60, 60, 60]\n",
      "2025-06-17 17:22:33,604 | INFO | models.fno.model | ✅ Coordinate grid generated.\n",
      "2025-06-17 17:22:33,604 | INFO | models.fno.model | 🔗 Added grid to input. New shape: torch.Size([32, 4, 60, 60, 60])\n",
      "2025-06-17 17:22:34,214 | INFO | models.fno.model | 🚀 FNO forward pass started. Input shape: torch.Size([16, 1, 60, 60, 60])\n",
      "2025-06-17 17:22:34,217 | INFO | models.fno.model | 🌐 Generating coordinate grid with shape: [60, 60, 60]\n",
      "2025-06-17 17:22:34,219 | INFO | models.fno.model | ✅ Coordinate grid generated.\n",
      "2025-06-17 17:22:34,219 | INFO | models.fno.model | 🔗 Added grid to input. New shape: torch.Size([16, 4, 60, 60, 60])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed with batch_size=32: CUDA out of memory. Tried to allocate 6.59 GiB. GPU 0 has a total capacity of 15.73 GiB of which 5.52 GiB is free. Process 12502 has 26.06 MiB memory in use. Including non-PyTorch memory, this process has 10.18 GiB memory in use. Of the allocated memory 9.95 GiB is allocated by PyTorch, and 85.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 17:22:34,577 | INFO | models.fno.model | 🚀 FNO forward pass started. Input shape: torch.Size([8, 1, 60, 60, 60])\n",
      "2025-06-17 17:22:34,578 | INFO | models.fno.model | 🌐 Generating coordinate grid with shape: [60, 60, 60]\n",
      "2025-06-17 17:22:34,579 | INFO | models.fno.model | ✅ Coordinate grid generated.\n",
      "2025-06-17 17:22:34,580 | INFO | models.fno.model | 🔗 Added grid to input. New shape: torch.Size([8, 4, 60, 60, 60])\n",
      "2025-06-17 17:22:34,669 | INFO | models.fno.model | 🔁 Passed through Fourier layer 1/4\n",
      "2025-06-17 17:22:34,674 | INFO | models.fno.model | 🔁 Passed through Fourier layer 2/4\n",
      "2025-06-17 17:22:34,679 | INFO | models.fno.model | 🔁 Passed through Fourier layer 3/4\n",
      "2025-06-17 17:22:34,683 | INFO | models.fno.model | 🔁 Passed through Fourier layer 4/4\n",
      "2025-06-17 17:22:34,684 | INFO | models.fno.model | ✅ Forward pass completed. Output shape: torch.Size([8, 1, 60, 60, 60])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed with batch_size=16: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 15.73 GiB of which 1.52 GiB is free. Process 12502 has 26.06 MiB memory in use. Including non-PyTorch memory, this process has 14.18 GiB memory in use. Of the allocated memory 12.53 GiB is allocated by PyTorch, and 1.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "✅ Success with batch_size=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 17:22:37,603 | INFO | models.fno.model | 🚀 FNO forward pass started. Input shape: torch.Size([4, 1, 60, 60, 60])\n",
      "2025-06-17 17:22:37,606 | INFO | models.fno.model | 🌐 Generating coordinate grid with shape: [60, 60, 60]\n",
      "2025-06-17 17:22:37,607 | INFO | models.fno.model | ✅ Coordinate grid generated.\n",
      "2025-06-17 17:22:37,608 | INFO | models.fno.model | 🔗 Added grid to input. New shape: torch.Size([4, 4, 60, 60, 60])\n",
      "2025-06-17 17:22:37,657 | INFO | models.fno.model | 🔁 Passed through Fourier layer 1/4\n",
      "2025-06-17 17:22:37,660 | INFO | models.fno.model | 🔁 Passed through Fourier layer 2/4\n",
      "2025-06-17 17:22:37,663 | INFO | models.fno.model | 🔁 Passed through Fourier layer 3/4\n",
      "2025-06-17 17:22:37,666 | INFO | models.fno.model | 🔁 Passed through Fourier layer 4/4\n",
      "2025-06-17 17:22:37,668 | INFO | models.fno.model | ✅ Forward pass completed. Output shape: torch.Size([4, 1, 60, 60, 60])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success with batch_size=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 17:22:40,307 | INFO | models.fno.model | 🚀 FNO forward pass started. Input shape: torch.Size([2, 1, 60, 60, 60])\n",
      "2025-06-17 17:22:40,309 | INFO | models.fno.model | 🌐 Generating coordinate grid with shape: [60, 60, 60]\n",
      "2025-06-17 17:22:40,311 | INFO | models.fno.model | ✅ Coordinate grid generated.\n",
      "2025-06-17 17:22:40,312 | INFO | models.fno.model | 🔗 Added grid to input. New shape: torch.Size([2, 4, 60, 60, 60])\n",
      "2025-06-17 17:22:40,318 | INFO | models.fno.model | 🔁 Passed through Fourier layer 1/4\n",
      "2025-06-17 17:22:40,323 | INFO | models.fno.model | 🔁 Passed through Fourier layer 2/4\n",
      "2025-06-17 17:22:40,327 | INFO | models.fno.model | 🔁 Passed through Fourier layer 3/4\n",
      "2025-06-17 17:22:40,332 | INFO | models.fno.model | 🔁 Passed through Fourier layer 4/4\n",
      "2025-06-17 17:22:40,334 | INFO | models.fno.model | ✅ Forward pass completed. Output shape: torch.Size([2, 1, 60, 60, 60])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success with batch_size=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 17:22:42,865 | INFO | models.fno.model | 🚀 FNO forward pass started. Input shape: torch.Size([1, 1, 60, 60, 60])\n",
      "2025-06-17 17:22:42,867 | INFO | models.fno.model | 🌐 Generating coordinate grid with shape: [60, 60, 60]\n",
      "2025-06-17 17:22:42,868 | INFO | models.fno.model | ✅ Coordinate grid generated.\n",
      "2025-06-17 17:22:42,870 | INFO | models.fno.model | 🔗 Added grid to input. New shape: torch.Size([1, 4, 60, 60, 60])\n",
      "2025-06-17 17:22:42,893 | INFO | models.fno.model | 🔁 Passed through Fourier layer 1/4\n",
      "2025-06-17 17:22:42,898 | INFO | models.fno.model | 🔁 Passed through Fourier layer 2/4\n",
      "2025-06-17 17:22:42,902 | INFO | models.fno.model | 🔁 Passed through Fourier layer 3/4\n",
      "2025-06-17 17:22:42,906 | INFO | models.fno.model | 🔁 Passed through Fourier layer 4/4\n",
      "2025-06-17 17:22:42,908 | INFO | models.fno.model | ✅ Forward pass completed. Output shape: torch.Size([1, 1, 60, 60, 60])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success with batch_size=1\n"
     ]
    }
   ],
   "source": [
    "def test_batch_size(batch_size):\n",
    "    try:\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                _ = model(x)\n",
    "                print(f\"✅ Success with batch_size={batch_size}\")\n",
    "                break\n",
    "    except RuntimeError as e:\n",
    "        print(f\"❌ Failed with batch_size={batch_size}: {str(e).splitlines()[0]}\")\n",
    "\n",
    "for bs in [32, 16, 8, 4, 2, 1]:\n",
    "    test_batch_size(bs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2ac54ce-3ade-4ecd-9994-9cf5823959ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MSE Loss on sample batch: 87.4357\n"
     ]
    }
   ],
   "source": [
    "from shared.losses import mse_loss, spectral_loss\n",
    "\n",
    "loss_val = mse_loss(x.to(device), y.to(device))\n",
    "print(f\"✅ MSE Loss on sample batch: {loss_val.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc3834fd-c938-4697-80ce-c311fc22d92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Optimizer and LR scheduler initialized.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "print(\"✅ Optimizer and LR scheduler initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1de346c0-7eb5-4715-9a9d-149762f80ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/27648 [00:00<?, ?it/s]2025-06-17 17:22:47,990 | INFO | models.fno.model | 🚀 FNO forward pass started. Input shape: torch.Size([4, 1, 60, 60, 60])\n",
      "2025-06-17 17:22:47,991 | INFO | models.fno.model | 🌐 Generating coordinate grid with shape: [60, 60, 60]\n",
      "2025-06-17 17:22:47,992 | INFO | models.fno.model | ✅ Coordinate grid generated.\n",
      "2025-06-17 17:22:47,993 | INFO | models.fno.model | 🔗 Added grid to input. New shape: torch.Size([4, 4, 60, 60, 60])\n",
      "Epoch 1:   0%|          | 0/27648 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 15.73 GiB of which 1.52 GiB is free. Process 12502 has 26.06 MiB memory in use. Including non-PyTorch memory, this process has 14.18 GiB memory in use. Of the allocated memory 12.48 GiB is allocated by PyTorch, and 1.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m inputs, targets = inputs.to(device), targets.to(device)\n\u001b[32m     13\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m loss = mse_loss(outputs, targets)\n\u001b[32m     16\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/caefs/user/mmingyeong/_dm2ics_model_benchmark/dm2ics_model_benchmark/models/fno/model.py:137\u001b[39m, in \u001b[36mFNO.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m# Fourier layers\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.fourier_blocks):\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m🔁 Passed through Fourier layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.fourier_blocks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    140\u001b[39m \u001b[38;5;66;03m# Projection\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/caefs/user/mmingyeong/_dm2ics_model_benchmark/dm2ics_model_benchmark/models/fno/layers/spectral_convolution.py:233\u001b[39m, in \u001b[36mSpectralConvolution.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    228\u001b[39m     out_ft_real, out_ft_imag = \u001b[38;5;28mself\u001b[39m.mix_weights(\n\u001b[32m    229\u001b[39m         out_ft_real, out_ft_imag, x_ft_real, x_ft_imag, \u001b[38;5;28mself\u001b[39m.weights_real, \u001b[38;5;28mself\u001b[39m.weights_imag\n\u001b[32m    230\u001b[39m     )\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.factorization == \u001b[33m'\u001b[39m\u001b[33mtucker\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    232\u001b[39m     \u001b[38;5;66;03m# Reconstruct weights from Tucker factorization and use them directly\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m     out_ft_real, out_ft_imag = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmix_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mout_ft_real\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_ft_imag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_ft_real\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_ft_imag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtucker_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcore_real\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfactors_real\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtucker_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcore_imag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfactors_imag\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.factorization == \u001b[33m'\u001b[39m\u001b[33mcp\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    239\u001b[39m     \u001b[38;5;66;03m# Reconstruct weights from CP factorization and use them directly\u001b[39;00m\n\u001b[32m    240\u001b[39m     out_ft_real, out_ft_imag = \u001b[38;5;28mself\u001b[39m.mix_weights(\n\u001b[32m    241\u001b[39m         out_ft_real, out_ft_imag, x_ft_real, x_ft_imag,\n\u001b[32m    242\u001b[39m         tl.cp_to_tensor((\u001b[38;5;28mself\u001b[39m.weights_cp_real, [factor \u001b[38;5;28;01mfor\u001b[39;00m factor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.factors_cp_real])),\n\u001b[32m    243\u001b[39m         tl.cp_to_tensor((\u001b[38;5;28mself\u001b[39m.weights_cp_imag, [factor \u001b[38;5;28;01mfor\u001b[39;00m factor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.factors_cp_imag]))\n\u001b[32m    244\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/caefs/user/mmingyeong/_dm2ics_model_benchmark/dm2ics_model_benchmark/models/fno/layers/spectral_convolution.py:178\u001b[39m, in \u001b[36mSpectralConvolution.mix_weights\u001b[39m\u001b[34m(self, out_ft_real, out_ft_imag, x_ft_real, x_ft_imag, weights_real, weights_imag)\u001b[39m\n\u001b[32m    174\u001b[39m slices = \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mmin\u001b[39m(mode, x_ft_real.size(i + \u001b[32m2\u001b[39m))) \u001b[38;5;28;01mfor\u001b[39;00m i, mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.modes))\n\u001b[32m    176\u001b[39m \u001b[38;5;66;03m# Mix weights\u001b[39;00m\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# First weight\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m out_ft_real[(\u001b[38;5;28mEllipsis\u001b[39m,) + slices], out_ft_imag[(\u001b[38;5;28mEllipsis\u001b[39m,) + slices] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcomplex_mult\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx_ft_real\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mEllipsis\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mslices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_ft_imag\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mEllipsis\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mslices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweights_real\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mEllipsis\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mslices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_imag\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mEllipsis\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mslices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weights_real, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(weights_real) > \u001b[32m1\u001b[39m:\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# Remaining weights\u001b[39;00m\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(weights_real)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/caefs/user/mmingyeong/_dm2ics_model_benchmark/dm2ics_model_benchmark/models/fno/layers/spectral_convolution.py:112\u001b[39m, in \u001b[36mSpectralConvolution.complex_mult\u001b[39m\u001b[34m(input_real, input_imag, weights_real, weights_imag)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03mPerforms complex multiplication between input and weights.\u001b[39;00m\n\u001b[32m    101\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    109\u001b[39m \u001b[33;03m    Tuple[torch.Tensor, torch.Tensor]: Real and imaginary parts of the result. [batch_size, out_channels, *sizes]\u001b[39;00m\n\u001b[32m    110\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m out_real = torch.einsum(\u001b[33m'\u001b[39m\u001b[33mbi...,io...->bo...\u001b[39m\u001b[33m'\u001b[39m, input_real, weights_real) - torch.einsum(\u001b[33m'\u001b[39m\u001b[33mbi...,io...->bo...\u001b[39m\u001b[33m'\u001b[39m, input_imag, weights_imag)\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m out_imag = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbi...,io...->bo...\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_real\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_imag\u001b[49m\u001b[43m)\u001b[49m + torch.einsum(\u001b[33m'\u001b[39m\u001b[33mbi...,io...->bo...\u001b[39m\u001b[33m'\u001b[39m, input_imag, weights_real)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out_real, out_imag\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/2023.03/envs/py312/lib/python3.12/site-packages/torch/functional.py:407\u001b[39m, in \u001b[36meinsum\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, *_operands)\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) <= \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum.enabled:\n\u001b[32m    405\u001b[39m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[32m    406\u001b[39m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    409\u001b[39m path = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum.is_available():\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 15.73 GiB of which 1.52 GiB is free. Process 12502 has 26.06 MiB memory in use. Including non-PyTorch memory, this process has 14.18 GiB memory in use. Of the allocated memory 12.48 GiB is allocated by PyTorch, and 1.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model.train()\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "n_batch = 10\n",
    "\n",
    "for epoch in range(3):\n",
    "    total_loss = 0.0\n",
    "    for i, (inputs, targets) in enumerate(tqdm(loader, desc=f\"Epoch {epoch+1}\")):\n",
    "        if i >= n_batch:\n",
    "            break\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = mse_loss(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    print(f\"📉 Epoch {epoch+1} Loss: {total_loss / n_batch:.4f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f34bd0-297a-47bb-99f5-746fbd733181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Trainer 설정 (디버깅용)\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=2,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    log_every_n_steps=1,\n",
    "    enable_progress_bar=True,\n",
    "    detect_anomaly=True,\n",
    "    logger=CSVLogger(\"logs/debug_fno\", name=\"fno_test\"),\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=2, mode=\"min\", verbose=True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2a1fec-7c5b-4035-8ead-0697b70b60f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"fno_test_model.pt\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"✅ FNO model saved to {save_path}\")\n",
    "\n",
    "state_dict = torch.load(save_path, map_location='cpu')\n",
    "print(f\"🔍 저장된 키 개수: {len(state_dict)}\")\n",
    "print(\"예시 키:\", list(state_dict.keys())[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a209466-38d9-4d97-aef8-019b6cbb0d02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py312)",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
