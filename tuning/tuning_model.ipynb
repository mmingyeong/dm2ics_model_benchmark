{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa055be-ace8-42cb-96f9-3bc8adfcc710",
   "metadata": {},
   "source": [
    "# 🔧 Hyperparameter Tuning Pipeline\n",
    "\n",
    "1. [ ] 정의할 하이퍼파라미터 설정\n",
    "   - 예: learning rate, batch size, weight decay, dropout rate, model depth 등\n",
    "\n",
    "2. [ ] 검색 공간(Search Space) 정의\n",
    "   - grid search, random search, 또는 optuna 등으로 범위 설정\n",
    "\n",
    "3. [ ] 실험 반복 구조 구현\n",
    "   - 각 하이퍼파라미터 조합마다 학습, 검증 평가 루프 구성\n",
    "\n",
    "4. [ ] 성능 기준 정의\n",
    "   - 예: validation loss, accuracy, F1 score, power spectrum error 등\n",
    "\n",
    "5. [ ] 결과 기록 및 정리\n",
    "   - 각 실험마다 config, 성능 metric, log, 시각화 등을 자동 저장\n",
    "\n",
    "6. [ ] 최적 조합 선택 및 분석\n",
    "   - best config 선택 + 중요도 분석 (e.g., via optuna’s feature importance)\n",
    "\n",
    "7. [ ] 선택된 설정으로 full training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c589206e-f69f-4714-9d6e-76c5d571842b",
   "metadata": {},
   "source": [
    "딥러닝 모델에서 공통적으로 **튜닝 대상이 되는 하이퍼파라미터**들을 아래와 같이 분류해 정리할 수 있습니다. 이 리스트는 대부분의 딥러닝 모델—CNN, Transformer, GAN, U-Net, FNO 등—에 **공통적으로 적용 가능**한 요소들입니다.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 딥러닝 모델 공통 하이퍼파라미터 튜닝 목록\n",
    "\n",
    "#### 📌 **Optimization 관련**\n",
    "\n",
    "* [ ] **Learning Rate** (`lr`): 학습 속도. 가장 중요한 하이퍼파라미터 중 하나.\n",
    "* [ ] **Optimizer Type**: `Adam`, `AdamW`, `SGD`, `RMSprop` 등\n",
    "* [ ] **Weight Decay**: L2 regularization 계수\n",
    "* [ ] **Momentum** (SGD 계열): 관성 효과 조절\n",
    "* [ ] **Gradient Clipping**: gradient exploding 방지\n",
    "\n",
    "#### 📌 **Batch 및 Epoch 관련**\n",
    "\n",
    "* [ ] **Batch Size**: 메모리, 학습 안정성, 일반화에 영향\n",
    "* [ ] **Number of Epochs**: 학습 반복 횟수\n",
    "* [ ] **Early Stopping Patience**: 과적합 방지를 위한 조기 종료 기준\n",
    "\n",
    "#### 📌 **Learning Rate Scheduler**\n",
    "\n",
    "* [ ] **Scheduler Type**: `StepLR`, `CosineAnnealing`, `ReduceLROnPlateau`, `CyclicLR` 등\n",
    "* [ ] **Scheduler Parameters**: step size, gamma, warm-up steps 등\n",
    "\n",
    "#### 📌 **Model Architecture 관련**\n",
    "\n",
    "* [ ] **Model Depth**: 레이어 수 (e.g., residual blocks, transformer layers)\n",
    "* [ ] **Hidden Size / Channels**: 레이어당 feature 수 (e.g., `n_gf`, `dim`, `channels`)\n",
    "* [ ] **Dropout Rate**: overfitting 방지를 위한 regularization\n",
    "* [ ] **Activation Function**: ReLU, LeakyReLU, GELU, Mish 등\n",
    "\n",
    "#### 📌 **Loss Function 관련**\n",
    "\n",
    "* [ ] **Loss Weighting**: 다양한 손실 함수 간 가중치 (e.g., MSE + SpectralLoss)\n",
    "* [ ] **Label Smoothing**: classification 계열에서 정답 분포를 부드럽게\n",
    "\n",
    "#### 📌 **Normalization 및 Regularization**\n",
    "\n",
    "* [ ] **Normalization Type**: BatchNorm, InstanceNorm, GroupNorm 등\n",
    "* [ ] **Dropout Position 및 적용 여부**\n",
    "* [ ] **Data Augmentation 강도**: (주로 이미지 계열)\n",
    "\n",
    "#### 📌 **AMP 및 Precision**\n",
    "\n",
    "* [ ] **Mixed Precision 사용 여부**: AMP on/off (`torch.amp`)\n",
    "* [ ] **Gradient Accumulation Steps**: 작은 배치일 때 유용\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 참고: 실험 설계 시 고려사항\n",
    "\n",
    "* 파라미터 간 상호작용이 있으므로 **하나씩 순차적으로 조정**하거나, Optuna 등으로 **자동화된 탐색**을 추천.\n",
    "* 특히 `learning rate`, `batch size`, `model size`는 **강하게 연관됨** → 함께 고려해야 함.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852b51f0-2280-4afd-a460-09cc0b1d84a8",
   "metadata": {},
   "source": [
    "Optuna는 딥러닝 및 머신러닝 모델의 하이퍼파라미터 튜닝을 자동화해주는 파이썬 기반 오픈소스 라이브러리입니다. 간단히 말해:\n",
    "\n",
    "🔍 “가장 좋은 하이퍼파라미터 조합을 자동으로 찾아주는 똑똑한 실험 스케줄러”입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275b462b-731c-451b-9070-1c40eca43f6a",
   "metadata": {},
   "source": [
    "✅ Optuna로 찾을 수 있는 하이퍼파라미터\n",
    "이들은 수치적으로 표현되고, 명확하게 평가 가능하며, 주어진 범위에서 반복적으로 실험할 수 있는 파라미터들입니다:\n",
    "    \n",
    "분류\t예시\n",
    "\n",
    "학습 관련\tlearning_rate, batch_size, weight_decay, dropout_rate\n",
    "\n",
    "모델 구조\tnum_layers, hidden_dim, kernel_size, activation (ReLU vs. Mish 등)\n",
    "\n",
    "Regularization\tdropout, L2 penalty, label smoothing\n",
    "\n",
    "Optimizer 종류 및 파라미터\toptimizer (Adam vs. SGD), beta1, momentum\n",
    "\n",
    "Loss 구성 비율\tGAN에서 λ_fm, λ_cc, λ_l1, Spectral loss weight 등\n",
    "\n",
    "데이터 증강\t증강 여부, 확률, 강도 (aug_prob=0.3, noise_std=0.01 등)\n",
    "\n",
    "기타\tscheduler 유형 및 decay step/ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a37f22-a1a6-4af4-9b0e-3e0264280fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py312)",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
